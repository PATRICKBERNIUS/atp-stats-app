---
title: "ATP Data Scraper"
format: gfm
---

Main link below:

https://www.atptour.com/en/-/www/StatsLeaderboard/serve/52week/all/all/false?v=1


```{python}
import pandas as pd
import requests
import time
from itertools import product
from requests.exceptions import ConnectionError, HTTPError, Timeout
from tqdm import tqdm


#defining category variables
stat_types = ['serve', 'return', 'pressure']
years = list(range(1991, 2026)) # 1991 - 1995
time_frame = ['52week', 'career'] + [str(year) for year in years]
surfaces = ['Clay', 'Grass', 'Hard', 'all']
vs_ranks = ['all', 'Top10', 'Top20', 'Top50']
former_no1_flag = ['false']

#every combination for URLs
all_combos = list(product(stat_types, time_frame, surfaces, vs_ranks, former_no1_flag)) 


#Requesting stats from website

#browser header
headers = {'User-Agent': 'Mozilla/5.0'}

all_dfs = [] #for fetched data
missing_data = [] #for missing data


#loops through each combination of variables and creates the url
for combo in tqdm(all_combos, desc='Fetching Leaderboard Data'):
    stat, time_val, surf, vs, no1 = combo
    url = f"https://www.atptour.com/en/-/www/StatsLeaderboard/{stat}/{time_val}/{surf}/{vs}/{no1}?v=1"

    #attempts to fetch the url
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()  # Checks if request was successful. Raises HTTPError if the status is 4xx or 5xx

        data = response.json()
        leaderboard = data.get('Leaderboard') #gets the leaderboard key from the json

        if leaderboard:
            df = pd.json_normalize(leaderboard) #flattens the leaderboard data

            #flattens nested stats data
            if 'stats' in df.columns:
                stats_df = pd.json_normalize(df['stats'])
                df=pd.concat([df.drop(columns='stats'), stats_df], axis=1) #combines the two dataframes

            #renaming columns
            df['stat'] = stat
            df['time'] = time_val
            df['surface'] = surf
            df['vs_rank'] = vs
            df['former_no1'] = no1

            all_dfs.append(df)

        #catches errors
        else:
            print(f"No leaderboard found: {url}")
            missing_data.append((stat, time_val, surf, vs, no1))
    except (ConnectionError, HTTPError, Timeout) as e:
        print(f"Request error at: {url} - {e}")
        missing_data.append((stat, time_val, surf, vs, no1))
        time.sleep(0.5)
        

#combines all the dataframes. Resets the index
full_df = pd.concat(all_dfs, ignore_index=True)


```

```{python}
#Dropping unneaded columns

full_df_clean = full_df.drop(columns=['ScRelativeUrlPlayerProfile', 'ScRelativeUrlPlayerCountryFlag', 'PlayerWasThisYearEoyNumberOne', 'EventYearEoyNumberOne', 'PartnerId', 'PartnerName', 'PartnerCountryCode', 'former_no1'])

full_df_clean
```

```{python}
#Separating the full_df by stat type

serve_df = full_df_clean[full_df_clean['stat'] == 'serve']
return_df = full_df_clean[full_df_clean['stat'] == 'return']
pressure_df = full_df_clean[full_df_clean['stat'] == 'pressure']
```

```{python}
#dropping empty columns I.E. return data in serve table

serve_df.dropna(axis=1, inplace=True)
return_df.dropna(axis=1, inplace=True)
pressure_df.dropna(axis=1, inplace=True)
```

```{python}
#cleaning column names

for df in [serve_df, return_df, pressure_df]:
    df.drop(columns=[col for col in df.columns if col.endswith('SortField')], inplace=True)
    df.columns = df.columns.str.replace('Stats.', '', regex=False)
```

```{python}
#Changing numeric data to float data type

def rename_numeric_columns(df):
    for col in df.columns:
        if df[col].dtype == 'object':
            df[col] = df[col].str.replace('%', '').str.strip()
            try:
                df[col] = df[col].astype(float)
            except ValueError:
                pass
    return df


rename_numeric_columns(serve_df)
rename_numeric_columns(return_df)
rename_numeric_columns(pressure_df)
```

```{python}
serve_df
```

```{python}
# extracting bio information for each player on atp website
import pandas as pd
import requests
import re
import time
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed

#browser headers
HEADERS = {'User-Agent' : 'Mozilla/5.0'}

#list of players ids to fetch for URL
player_ids = full_df['PlayerId'].unique()

#retries in case of failure
max_retries = 3

def get_player_bio(player_id):
    bio_url= f'https://www.atptour.com/en/-/www/players/hero/{player_id}?v=1'
    for attempt in range(max_retries):
        try:
            response_2 = requests.get(bio_url, headers=HEADERS, timeout=10)
            response_2.raise_for_status()
            return {'bio' : response_2.json(), 'player_id' : player_id, 'success' : True}
        except requests.exceptions.RequestException as e:
            print(f'Attempt {attempt+1} failed for {player_id}: {e}')
            time.sleep(2 ** attempt) #increase wait time if multiple attempts
    print(f'Giving up on player id {player_id}')
    return {'bio' : None, 'player_id' : player_id, 'success' : False}



bios = []
failed_ids = []

#12 parallel threads
with ThreadPoolExecutor(max_workers=12) as executor: 
    futures = [executor.submit(get_player_bio, pid) for pid in player_ids] #runs function on each playerID
    for future in tqdm(as_completed(futures), total=len(futures), desc='Fetching Bios ATP'):
        result = future.result()
        if result['success']:
            bios.append(result['bio'])
        else:
            failed_ids.append(result['player_id'])
    time.sleep(0.5)

bio_df = pd.DataFrame(bios)
print(bio_df)
```


```{python}
bio_df
```



```{python}
#cleaning bio dataframe
bio_df_columns_to_keep = ['LastName', 'FirstName', 'BirthDate', 'Age', 'NatlId', 'Nationality', 'HeightFt', 'HeightIn', 'HeightCm', 'WeightLb', 'WeightKg', 'PlayHand', 'BackHand', 'ProYear', 'Active', 'SglHiRank', 'CareerPrizeFormatted', ]

existing_columns = [col for col in bio_df_columns_to_keep if col in bio_df.columns]


bio_df_filtered = bio_df[existing_columns]

# Combine first + last name
bio_df_filtered['PlayerName'] = bio_df_filtered['FirstName'] + ' ' + bio_df_filtered['LastName'].str.strip()
bio_df_filtered = bio_df_filtered.drop(columns=['LastName', 'FirstName'])

# Convert birthdate to proper datetime
bio_df_filtered['BirthDate'] = pd.to_datetime(bio_df_filtered['BirthDate']).dt.date


# Extract descriptions from nested objects
bio_df_filtered['PlayHand'] = bio_df_filtered['PlayHand'].apply(lambda x : x.get('Description') if isinstance(x, dict) else None)
bio_df_filtered['BackHand'] = bio_df_filtered['BackHand'].apply(lambda x : x.get('Description') if isinstance(x, dict) else None)
bio_df_filtered['Active'] = bio_df_filtered['Active'].apply(lambda x : x.get('Description') if isinstance(x, dict) else None)


# Convert numbers with commas or currency to float
numeric_cols = ['Age', 'HeightIn', 'HeightCm', 'WeightLb', 'WeightKg', 'SglHiRank', 'CareerPrizeFormatted']

for col in numeric_cols:
    if bio_df_filtered[col].dtype == 'object':
        bio_df_filtered[col] = bio_df_filtered[col].str.replace(',', '')
        bio_df_filtered[col] = bio_df_filtered[col].str.replace('$', '')
        bio_df_filtered[col] = bio_df_filtered[col].astype(float)


#strip white space
full_df_clean['PlayerName'] = full_df_clean['PlayerName'].str.strip()
bio_df_filtered['PlayerName'] = bio_df_filtered['PlayerName'].str.strip()

#dropping duplicate instances
player_id_lookup = full_df_clean[['PlayerName', 'PlayerId']].drop_duplicates()

#merge bio table with playerIDs
bio_df_filtered = bio_df_filtered.merge(player_id_lookup, on= 'PlayerName', how= 'left')




#reorder columns
bio_df_filtered = bio_df_filtered[['PlayerName', 'PlayerId', 'BirthDate', 'Age', 'NatlId', 'Nationality', 'HeightFt', 'HeightIn',
       'HeightCm', 'WeightLb', 'WeightKg', 'PlayHand', 'BackHand', 'ProYear',
       'Active', 'SglHiRank', 'CareerPrizeFormatted', ]]

bio_df_filtered
```


```{python}
lookup_df = bio_df_filtered
lookup_df
```

```{python}
from selenium import webdriver #controll webpage 
from selenium.webdriver.chrome.service import Service #communicate with Chrome
from selenium.webdriver.common.by import By #used to specify locating elements
from selenium.webdriver.chrome.options import Options #configure chrome settings
from webdriver_manager.chrome import ChromeDriverManager #downloads and manages ChromeDriver


#set Chrome to run in headless mode. Does not open a browser
options = Options()
options.add_argument('--headless')

#downloads the correct ChromeDriver version. Insures headless browser
driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)


driver.get('https://www.atptour.com/en/stats/win-loss-index?indexCategory=all&timeCategory=career&country=all&sortBy=index&sortDirection=desc')

#finds where id = country on the page after it has been rendered by javascript
country_dropdown = driver.find_element(By.ID, 'country')

#finds each option tag within 'country' and collects the values i.e. country abv
options = country_dropdown.find_elements(By.TAG_NAME, 'option')

#loops through each option and extracts each 'data-value' attribute and assigns it to a list
country_codes = [opt.get_attribute('data-value') for opt in options]

#closes the browser
driver.quit()


from concurrent.futures import ThreadPoolExecutor, as_completed

timeCategory = ['roll', 'career', 'ytd']
indexCategory = ['all', 'after1stsetlost', 'after1stsetwin', 'finalset', '5thset', 'finals', 'grandslam', 'indoor', 'vslefthanders', 'vsrighthanders', '1000', 'carpet', 'hard', 'grass', 'clay', 'outdoor', 'tiebreak', 'vstop10']
country = country_codes


combinations = list(product(indexCategory, timeCategory, country))

max_retries = 2

all_data = []
skipped_combinations = []

#function to get win/loss date from ATP
def fetchData(combo):
    category, time_period, cntry = combo
    win_loss_url = f'https://www.atptour.com/en/-/www/stats/winloss//{category}/{time_period}/{cntry}/index/desc/1/1000?v=1'
    
    for attempt in range(1, max_retries + 1):
        try:
            response_3 = requests.get(win_loss_url, headers=headers, timeout=10)
            response_3.raise_for_status()
            json_data = response_3.json()

            if not json_data:
                return [], (category, time_period, cntry)
            for player in json_data:
                player['Category'] = category
                player['TimePeriod'] = time_period
                player['Country'] = cntry
            return json_data, None

        except requests.RequestException as e:
            if attempt == max_retries:
                return [], (category, time_period, cntry)
            time.sleep(0.5 * attempt)


#run function with parallel workers with loading bar
with ThreadPoolExecutor(max_workers=12) as executor:
    futures = [executor.submit(fetchData, combo) for combo in combinations]
    for future in tqdm(as_completed(futures), total=len(combinations), desc="Fetching with Threads"):
        data, skipped = future.result()
        if data:
            all_data.extend(data)
        if skipped:
            skipped_combinations.append(skipped)

win_loss_df = pd.DataFrame(all_data)

if skipped_combinations:
    print('Some combos failed')
    print(skipped_combinations)




```

```{python}
win_loss_df
```


```{python}
#check for missing values
win_loss_df['Index'].isnull().any()
```
```{python}
#clean player names
win_loss_df['PlayerName'] = win_loss_df['FirstName'] + ' ' + win_loss_df['LastName'].str.strip()
win_loss_df_2 = win_loss_df[['PlayerName', 'PlayerId', 'NatlId', 'Index', 'Titles', 'Win', 'Loss', 'Category', 'TimePeriod', 'Country']]
win_loss_df_2
```


```{python}
win_loss_df_2
```



```{python}
#saving dataframes to csvs
serve_df.to_csv('atp_serve_data.csv', index=False)
return_df.to_csv('atp_return_data.csv', index=False)
pressure_df.to_csv('atp_pressure_data.csv', index=False)
lookup_df.to_csv('atp_lookup.csv', index=False)
win_loss_df_2.to_csv('atp_win_loss_index.csv', index=False)

```





```{python}
#stats
#test for just aces since this code will take hours to run.
fact_type = ['Aces'] #, '1st-Serve', '1st-Serve-Points-Won', '2nd-Serve-Points-Won', 'Service-Games-Won', 'Break-Points-Saved', '1st-Serve-Return-Points-Won', '2nd-Serve-Return-Points-Won', 'Break-Points-Converted', 'Return-Games-Won']
years = list(range(1991, 2026))
date = ['career'] + [str(year) for year in years]
surface = surfaces
countries = country_codes

all_combos = list(product(fact_type, date, surface, country_codes))

stats_list = []

for combo in tqdm(all_combos, desc="Fetching Stats Data"):
    stat, time, surface, country = combo
    url = f"https://www.atptour.com/en/-/www/individualmatchstats//{time}/{surface}/{country}/{stat}/percentage/desc/1/1000"

    # request the JSON
    response = requests.get(url, headers=headers)
    data = response.json()
    #stats_list.append(data)


# convert to DataFrame
df = pd.DataFrame(data["StatsList"])   # adjust depending on JSON structure

# preview
df
```


```{python}
from joblib import Parallel, delayed
from tqdm_joblib import tqdm_joblib

#stat categories
fact_type = ['Aces', '1st-Serve', '1st-Serve-Points-Won', '2nd-Serve-Points-Won', 'Service-Games-Won', 'Break-Points-Saved', '1st-Serve-Return-Points-Won', '2nd-Serve-Return-Points-Won', 'Break-Points-Converted', 'Return-Games-Won'] 
years = list(range(1991, 2026))
date = ['career'] + [str(year) for year in years]
surface = surfaces
countries = country_codes


all_combos = list(product(fact_type, date, surface, countries))

bad_countries = set()


session = requests.Session()
session.headers.update(headers)

#function for getting individual match statistics
def get_stats(stat, time, surface, country):
    if country in bad_countries:
        return None

    stats_url = f"https://www.atptour.com/en/-/www/individualmatchstats//{time}/{surface}/{country}/{stat}/percentage/desc/1/1000"
    try:
        response = session.get(stats_url, timeout=10)

        # ensuring response is JSON
        if "application/json" not in response.headers.get("Content-Type", "").lower():
            bad_countries.add(country)
            return None

        data = response.json()

        if "StatsList" in data and data["StatsList"]:
            df_temp = pd.json_normalize(data["StatsList"])
            df_temp["Stat"] = stat
            df_temp["Time"] = time
            df_temp["Surface"] = surface
            df_temp["Country"] = country
            return df_temp
        else:
            if time == "career":
                bad_countries.add(country)
        return None

    except Exception:
        return None

#parellel execution
results = Parallel(n_jobs=8, backend="loky", batch_size=1)(
    delayed(get_stats)(stat, time, surface, country) 
    for (stat, time, surface, country) in tqdm(all_combos, desc="Fetching Stats")
)

stats_list = [r for r in results if r is not None]

if stats_list:
    df = pd.concat(stats_list, ignore_index=True)
    print(df.shape)
    print(df.head())
else:
    df = pd.DataFrame()
    print("NO data collected")

```


```{python}
print(df.head())
print(df.dtypes)
```

```{python}
#converting to csv
df.to_csv('atp_player_stats.csv', index=False)
```

```{python}
df = pd.read_csv("data_files/atp_player_stats.csv")
```



```{python}
#Cleaning Stats Data

df['PlayerName'] = df['FirstName'] + ' ' + df['LastName']
df = df.drop(columns=['FirstName', 'LastName'])

```


    

